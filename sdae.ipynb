{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import h5py\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genes after subsetting: 5676\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#reading in files\n",
    "X = pd.read_hdf(\"data/tcga_mutation_train.h5\", \"expression\")\n",
    "Y = pd.read_hdf('data/tcga_mutation_train.h5', 'labels')\n",
    "\n",
    "X_holdout = pd.read_hdf(\"data/tcga_mutation_test_unlabeled.h5\", \"expression\")\n",
    "#L1000 subsetting\n",
    "l1000_file = open(\"L1000_clueio_genelist.txt\")\n",
    "l1000 = [i.strip() for i in l1000_file.readlines()]\n",
    "#get l1000 genes that are in data\n",
    "# L1000_= pd.Series(list(set(X.columns) & set(l1000)))\n",
    "# X_L1000 = X[L1000_] #subset X data\n",
    "\n",
    "# Prune expression to only KEGG pathway genes\n",
    "with open(\"data/c2.cp.kegg.v6.1.symbols.gmt\") as f:\n",
    "    genes_subset = list(set().union(*[line.strip().split(\"\\t\")[2:] for line in f.readlines()]))\n",
    "X_pruned = X.drop(labels=(set(X.columns) - set(genes_subset)), axis=1, errors=\"ignore\")\n",
    "\n",
    "#using both kegg and L1000\n",
    "subset_ = set(X_pruned.columns.tolist() + l1000)\n",
    "subset = pd.Series(list(set(X.columns) & set(subset_)))\n",
    "\n",
    "print(\"Number of genes after subsetting:\", len(subset))\n",
    "\n",
    "X_sub = X[subset] #subset X data\n",
    "X_sub_holdout =X_holdout[subset]\n",
    "\n",
    "\n",
    "x_array = np.array(X.values, dtype=np.float32)\n",
    "\n",
    "# extract sample id values\n",
    "y_names = list(set(Y[\"detailed_category\"].values))\n",
    "y_names = sorted(y_names)\n",
    "\n",
    "m,n = X.shape\n",
    "y_array = np.zeros(shape=(m, len(y_names)+3), dtype=np.float32)\n",
    "\n",
    "# create a key for id's to indices\n",
    "y_index_key = {name:i for i,name in enumerate(y_names)}\n",
    "# generate one-hot vectors for all id's\n",
    "for m,primary_site_name in enumerate(Y[\"detailed_category\"].values):\n",
    "    \n",
    "    index = y_index_key[primary_site_name]\n",
    "    y_array[m,index] = 1\n",
    "    \n",
    "    y_array[m, -3] = Y.iloc[m][6]\n",
    "    y_array[m, -2] = Y.iloc[m][7]\n",
    "    y_array[m, -1] = Y.iloc[m][8]\n",
    "    \n",
    "    \n",
    "#split to train + test\n",
    "xTrain, xTest, yTrain, yTest = sklearn.model_selection.train_test_split(X_sub, y_array, test_size=0.3, random_state=42)\n",
    "#split test to test and validate \n",
    "xTest, xValidate, yTest, yValidate = sklearn.model_selection.train_test_split(xTest, yTest, test_size=1/3, random_state=42)\n",
    "\n",
    "\n",
    "print(yTrain)\n",
    "xTrain = np.array(xTrain.values, dtype=np.float32)\n",
    "xTest = np.array(xTest.values, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderDataset(Dataset):\n",
    "    def __init__(self, x):\n",
    "        x_dtype = torch.FloatTensor\n",
    "\n",
    "        self.length = x.shape[0]\n",
    "\n",
    "        self.x_data = torch.from_numpy(x).type(x_dtype)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.x_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "class PredictorDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x_dtype = torch.FloatTensor\n",
    "        y_dtype = torch.FloatTensor\n",
    "        self.length = x.shape[0]\n",
    "\n",
    "        self.x_data = torch.from_numpy(x).type(x_dtype)\n",
    "        self.y_data = torch.from_numpy(y).type(y_dtype)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def addNoise(x):\n",
    "    '''set 1% of 1s to 0'''\n",
    "    noisy = [[0 if (y == 0 or (y * random.uniform(0,1)) <= 0.01) else 1 for y in vector]for vector in x]\n",
    "    return np.array(noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_batch(model, x, y, optimizer, loss_fn, layer):\n",
    "    # Run forward calculation\n",
    "    y_predict = model.forward(x, layer)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = loss_fn(y_predict, y)\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data[0]\n",
    "\n",
    "\n",
    "def train(model, x_in, y_in, optimizer, loss_fn, layer, epochs=20, batch_size=16):\n",
    "    losses = list()\n",
    "\n",
    "    batch_index = 0\n",
    "    for e in range(epochs):\n",
    "        x_new = addNoise(x_in)\n",
    "        dataset = PredictorDataset(x=x_new, y=y_in)\n",
    "        loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "        for x, y in loader:\n",
    "            x = Variable(x)\n",
    "            y = Variable(y)\n",
    "\n",
    "            loss = train_batch(model=model, x=x, y=y, optimizer=optimizer, loss_fn=loss_fn, layer=layer)\n",
    "            losses.append(loss)\n",
    "\n",
    "            batch_index += 1\n",
    "\n",
    "\n",
    "    return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(model, x, y):\n",
    "    # run forward calculation\n",
    "    y_predict = model.forward(x, 0)\n",
    "\n",
    "    return y, y_predict\n",
    "\n",
    "\n",
    "def test(model, loader):\n",
    "    y_vectors = list()\n",
    "    y_predict_vectors = list()\n",
    "\n",
    "    batch_index = 0\n",
    "    for x, y in loader:\n",
    "        x = Variable(x)\n",
    "        y = Variable(y)\n",
    "\n",
    "        y, y_predict = test_batch(model=model, x=x, y=y)\n",
    "\n",
    "        y_vectors.append(y.data.numpy())\n",
    "        y_predict_vectors.append(y_predict.data.numpy())\n",
    "\n",
    "        batch_index += 1\n",
    "\n",
    "    y_predict_vector = np.concatenate(y_predict_vectors)\n",
    "\n",
    "    return y_predict_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowLinear(nn.Module):\n",
    "    '''\n",
    "    A simple, general purpose, fully connected network\n",
    "    '''\n",
    "    def __init__(self, size1, size2, size3, trainData):\n",
    "        # Perform initialization of the pytorch superclass\n",
    "        super(ShallowLinear, self).__init__()\n",
    "        learning_rate = 1e-2\n",
    "        \n",
    "        # Define network layer dimensions\n",
    "        D_in, H1, H2, H3, D_out = [len(xTest[0]), size1, size2, size3, len(yTest[0])]    # These numbers correspond to each layer: [input, hidden_1, output]\n",
    "        \n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.decode1 = nn.Linear(H1, D_in)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.decode2 = nn.Linear(H2, H1)\n",
    "        self.linear3 = nn.Linear(H2, H3)\n",
    "        self.linear4 = nn.Linear(H3, D_out)\n",
    "\n",
    "    def forward(self, x, layer):\n",
    "        '''\n",
    "        This method defines the network layering and activation functions\n",
    "        '''\n",
    "        if layer == 0:\n",
    "            x = self.linear1(x) # hidden layer\n",
    "            x = F.relu(x)       # activation function\n",
    "\n",
    "            x = self.linear2(x) # output layer\n",
    "            x = F.relu(x)       # activation function\n",
    "\n",
    "            x = self.linear3(x) # output layer\n",
    "            x = F.relu(x)       # activation function\n",
    "\n",
    "            x = self.linear4(x) # output layer\n",
    "            \n",
    "        elif layer == 1:\n",
    "            \n",
    "            x = self.linear1(x) # hidden layer\n",
    "            x = F.relu(x)       # activation function\n",
    "\n",
    "            x = self.decode1(x) # output layer\n",
    "            x = F.relu(x)       # activation function\n",
    "            \n",
    "        elif layer == 2:\n",
    "            \n",
    "            x = self.linear2(x) # hidden layer\n",
    "            x = F.relu(x)       # activation function\n",
    "\n",
    "            x = self.decode2(x) # output layer\n",
    "            x = F.relu(x)       # activation function\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def encode(self, dataset, layer):\n",
    "        \"\"\"Return the trained hidden layer and encoded input\"\"\"\n",
    "        #Get the dataset\n",
    "        loader = DataLoader(dataset=dataset, batch_size=len(dataset), shuffle=False)\n",
    "        xs = []\n",
    "        for x, y in loader:\n",
    "            x = Variable(x)\n",
    "\n",
    "            if layer == 0:\n",
    "                x = self.linear1(x) # hidden layer\n",
    "                x = F.relu(x)       # activation function\n",
    "\n",
    "                x = self.linear2(x) # output layer\n",
    "                x = F.relu(x)       # activation function\n",
    "                \n",
    "            elif layer == 1:\n",
    "                x = self.linear1(x)\n",
    "                x = F.relu(x)\n",
    "\n",
    "            xs.append(x.data.numpy())\n",
    "        encoding = np.concatenate(xs)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(dataset_train, dataset_test, dataset_train_encode, size1, size2, size3, ae):\n",
    "    # Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "    batch_size_train = 32\n",
    "    \n",
    "    data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "    data_loader_train_encode = DataLoader(dataset=dataset_train_encode, batch_size=batch_size_train, shuffle=True)\n",
    "    data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False)\n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.MSELoss()  # mean squared error\n",
    "\n",
    "    # Train and get the resulting loss per iteration\n",
    "    #Train first autoencoder layer\n",
    "    #Create a new neural net for prediction\n",
    "    predictor_model = ShallowLinear(size1, size2, size3, xTrain)\n",
    "    if (ae):\n",
    "        '''if using autoencoder'''\n",
    "        \n",
    "        encoder_model = ShallowLinear(size1, size2, size3, xTrain)\n",
    "\n",
    "        # Initialize the optimizer with above parameters\n",
    "        optimizer = optim.SGD(encoder_model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        #Train the first autoencoder layer\n",
    "        train(model=encoder_model, x_in=xTrain, y_in=xTrain, optimizer=optimizer, layer = 1, loss_fn=loss_fn, epochs = 10)\n",
    "        \n",
    "        #Train second autoencoder layer\n",
    "        x_encoded = np.array(encoder_model.encode(dataset_train, 1))\n",
    "        train(model=encoder_model, x_in = x_encoded, y_in = x_encoded, optimizer=optimizer, layer = 2, loss_fn=loss_fn, epochs = 10)\n",
    "    \n",
    "    \n",
    "\n",
    "        #Copy parameters from autoencoding neural net to predicting neural net\n",
    "        mp = list(encoder_model.parameters())\n",
    "        mcp = list(predictor_model.parameters())\n",
    "        for i in range(len(mp)):\n",
    "            mp[i].data[:] = mcp[i].data[:]\n",
    "\n",
    "    #Train the predicting model\n",
    "    optimizer1 = optim.SGD(predictor_model.parameters(), lr=learning_rate) \n",
    "    loss = train(model=predictor_model, x_in=xTrain, y_in = yTrain, optimizer=optimizer1, layer = 0, loss_fn=loss_fn)\n",
    "    \n",
    "    # Test and get the resulting predicted y values\n",
    "    y_predict = test(model=predictor_model, loader=data_loader_test)\n",
    "\n",
    "    return loss, y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stacked autoencoder plus neural net. Two autoencoder layers with relu activation + one layer with relu activation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aeResults = []\n",
    "nnResults = []\n",
    "def testAE_NN():\n",
    "    print(\"Testing stacked autoencoder plus neural net. Two autoencoder layers with relu activation + one layer with relu activation\")\n",
    "    dataset_train = PredictorDataset(x=xTrain, y=yTrain)\n",
    "    dataset_test = PredictorDataset(x=xTest, y=yTest)\n",
    "    dataset_train_encode = AutoencoderDataset(xTrain)\n",
    "    sizes = [(1200, 1100), (int(len(xTest[0])*.75), int(len(xTest[0])/2)),\n",
    "             (int(len(xTest[0])/5), int(len(xTest[0])/10)), (int(len(xTest[0])/10), int(len(xTest[0])/15))]\n",
    "    for size in sizes:\n",
    "        size1, size2 = size\n",
    "        size3=size2\n",
    "\n",
    "        ### Train and predict with autoencoder\n",
    "        startTime = time.time()\n",
    "        losses, yPredict = run(dataset_train=dataset_train, dataset_test=dataset_test, dataset_train_encode = dataset_train_encode, \n",
    "                               size1=size1, size2=size2, size3=size3, ae = True)\n",
    "        elapsedTime = time.time() - startTime\n",
    "        yPred = np.array([[y_names[np.argmax(vector[0:-3])], 1 if vector[-3]>=0.5 else 0, 1 if vector[-2]>=0.5 else 0, 1 if vector[-1]>=0.5 else 0] \n",
    "                          for vector in yPredict])\n",
    "\n",
    "        yActual = np.array([[y_names[np.argmax(vector[0:-3])],1 if vector[-3]==1 else 0, 1 if vector[-2]==1 else 0, 1 if vector[-1]==1 else 0] for vector in yTest])\n",
    "\n",
    "        isCorrect = [[yPred[i][j] == yActual[i][j] for j in range(len(yPred[0]))] for i in range(len(yPred))]\n",
    "\n",
    "        \n",
    "        numCorrect = np.sum(isCorrect, axis=0)\n",
    "        \n",
    "        pred0, act0 = [v[0] for v in yPred], [v[0] for v in yActual]\n",
    "        pred1, act1 =  [int(v[1]) for v in yPred], [v[1] for v in yActual]\n",
    "        pred2, act2 =  [int(v[2]) for v in yPred], [v[2] for v in yActual]\n",
    "        pred3, act3 =  [int(v[3]) for v in yPred], [v[3] for v in yActual]\n",
    "        \n",
    "        ##### Write output file and get f1 score\n",
    "        pd.DataFrame({\n",
    "            \"TumorTypePrediction\": pred0,\n",
    "            \"TP53MutationPrediction\": pred1,\n",
    "            \"KRASMutationPrediction\": pred2,\n",
    "            \"BRAFMutationPrediction\": pred3,\n",
    "        }).to_csv(\"test_predictions.tsv\", sep=\"\\t\")\n",
    "\n",
    "        pd.DataFrame({\n",
    "            \"primary.disease.or.tissue\": act0,\n",
    "            \"TP53_mutant\": act1,\n",
    "            \"KRAS_mutant\": act2,\n",
    "            \"BRAF_mutant\": act3,\n",
    "        }).to_csv(\"test_actuals.tsv\", sep=\"\\t\")\n",
    "        \n",
    "        print(\"% Correct with autoencoder: \", size1, size2, size3, (numCorrect/len(isCorrect)))\n",
    "        print(\"Elapsed time %fs\" % elapsedTime)\n",
    "        !Rscript class/BME230_F1score_V2.R test_predictions.tsv test_actuals.tsv\n",
    "        \n",
    "        ### Train and predict without autoencoder\n",
    "        startTime = time.time()\n",
    "        losses, yPredict = run(dataset_train=dataset_train, dataset_test=dataset_test, dataset_train_encode = dataset_train_encode, \n",
    "                               size1=size1, size2=size2, size3=size3, ae = False)\n",
    "        elapsedTime = time.time() - startTime\n",
    "        yPred = np.array([[y_names[np.argmax(vector[0:-3])], 1 if vector[-3]>=0.5 else 0, 1 if vector[-2]>=0.5 else 0, 1 if vector[-1]>=0.5 else 0] \n",
    "                          for vector in yPredict])\n",
    "\n",
    "        yActual = np.array([[y_names[np.argmax(vector[0:-3])], 1 if vector[-3]==1 else 0, 1 if vector[-2]==1 else 0, 1 if vector[-1]==1 else 0] \n",
    "                            for vector in yTest])\n",
    "\n",
    "        isCorrect = [[yPred[i][j] == yActual[i][j] for j in range(len(yPred[0]))] for i in range(len(yPred))]\n",
    "\n",
    "        numCorrect = np.sum(isCorrect, axis=0)\n",
    "\n",
    "        pred0, act0 = [v[0] for v in yPred], [v[0] for v in yActual]\n",
    "        pred1, act1 =  [int(v[1]) for v in yPred], [v[1] for v in yActual]\n",
    "        pred2, act2 =  [int(v[2]) for v in yPred], [v[2] for v in yActual]\n",
    "        pred3, act3 =  [int(v[3]) for v in yPred], [v[3] for v in yActual]\n",
    "        \n",
    "        ##### Write output file and get f1 score\n",
    "        pd.DataFrame({\n",
    "            \"TumorTypePrediction\": pred0,\n",
    "            \"TP53MutationPrediction\": pred1,\n",
    "            \"KRASMutationPrediction\": pred2,\n",
    "            \"BRAFMutationPrediction\": pred3,\n",
    "        }).to_csv(\"test_predictions.tsv\", sep=\"\\t\")\n",
    "\n",
    "        pd.DataFrame({\n",
    "            \"primary.disease.or.tissue\": act0,\n",
    "            \"TP53_mutant\": act1,\n",
    "            \"KRAS_mutant\": act2,\n",
    "            \"BRAF_mutant\": act3,\n",
    "        }).to_csv(\"test_actuals.tsv\", sep=\"\\t\")\n",
    "        \n",
    "        print(\"% Correct without autoencoder: \", size1, size2, size3, (numCorrect/len(isCorrect)))\n",
    "        print(\"Elapsed time %f s\" % elapsedTime )\n",
    "        !Rscript class/BME230_F1score_V2.R test_predictions.tsv test_actuals.tsv\n",
    "        \n",
    "        \n",
    "\n",
    "testAE_NN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = PredictorDataset(x=xTrain, y=yTrain)\n",
    "dataset_train_encode = AutoencoderDataset(xTrain)\n",
    "\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_train_encode = DataLoader(dataset=dataset_train_encode, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "\n",
    "# Define the hyperparameters\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.MSELoss()  # mean squared error\n",
    "\n",
    "# Train and get the resulting loss per iteration\n",
    "#Train first autoencoder layer\n",
    "encoder_model = ShallowLinear(size1, size2, size2, xtrain)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.SGD(encoder_model.parameters(), lr=learning_rate)\n",
    "train(model=encoder_model, x_in=xtrain, y_in = xtrain, optimizer=optimizer, layer = 1, loss_fn=loss_fn, epochs = 30)\n",
    "#Train second autoencoder layer\n",
    "x_encoded = encoder_model.encode(dataset_train, 1)\n",
    "dataset_train_encode = AutoencoderDataset(x_encoded)\n",
    "data_loader_train_encode = DataLoader(dataset=dataset_train_encode, batch_size=batch_size_train, shuffle=True)\n",
    "train(model=encoder_model, x_in=x_encoded, y_in= x_encoded, optimizer=optimizer, layer = 2, loss_fn=loss_fn, epochs = 30)\n",
    "\n",
    "newXTrain = encoder_model.encode(dataset_train, 0) #Pass xTest through the encoder\n",
    "aeLogRegModel = OneVsRestClassifier(LogisticRegression()).fit(newXTrain, ytrain)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_test = AutoencoderDataset(x=xtest)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False)\n",
    "\n",
    "newXTest = encoder_model.encode(dataset_test, 0) #Pass xTest through the encoder\n",
    "y_predict = aeLogRegModel.predict(newXTest)\n",
    "\n",
    "yPred = np.array([[y_names[np.argmax(vector[0:-3])], 1 if vector[-3]>=0.5 else 0, 1 if vector[-2]>=0.5 else 0, 1 if vector[-1]>=0.5 else 0] for vector in yPred])\n",
    "    \n",
    "pred0 = [v[0] for v in yPred]\n",
    "pred1 =  [int(v[1]) for v in yPred]\n",
    "pred2 =  [int(v[2]) for v in yPred]\n",
    "pred3 =  [int(v[3]) for v in yPred]\n",
    "\n",
    "##### Write output file and get f1 score\n",
    "pd.DataFrame({\n",
    "    \"TumorTypePrediction\": pred0,\n",
    "    \"TP53MutationPrediction\": pred1,\n",
    "    \"KRASMutationPrediction\": pred2,\n",
    "    \"BRAFMutationPrediction\": pred3,\n",
    "}).to_csv(\"predict.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "results logreg with autoencoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "def run_logreg(xtrain, ytrain, xtest, ytest, size1, size2, multi):\n",
    "    \n",
    "    dataset_train = PredictorDataset(x=xtrain, y=ytrain)\n",
    "    dataset_test = PredictorDataset(x=xtest, y=ytest)\n",
    "    dataset_train_encode = AutoencoderDataset(xtrain)\n",
    "    \n",
    "    \n",
    "    # Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "    batch_size_train = 16\n",
    "    \n",
    "    data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "    data_loader_train_encode = DataLoader(dataset=dataset_train_encode, batch_size=batch_size_train, shuffle=True)\n",
    "    data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False)\n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.MSELoss()  # mean squared error\n",
    "\n",
    "    # Train and get the resulting loss per iteration\n",
    "    #Train first autoencoder layer\n",
    "    encoder_model = ShallowLinear(size1, size2, size2, xtrain)\n",
    "\n",
    "    # Initialize the optimizer with above parameters\n",
    "    optimizer = optim.SGD(encoder_model.parameters(), lr=learning_rate)\n",
    "    train(model=encoder_model, x_in=xtrain, y_in = xtrain, optimizer=optimizer, layer = 1, loss_fn=loss_fn, epochs = 30)\n",
    "    #Train second autoencoder layer\n",
    "    x_encoded = encoder_model.encode(dataset_train, 1)\n",
    "    dataset_train_encode = AutoencoderDataset(x_encoded)\n",
    "    data_loader_train_encode = DataLoader(dataset=dataset_train_encode, batch_size=batch_size_train, shuffle=True)\n",
    "    train(model=encoder_model, x_in=x_encoded, y_in= x_encoded, optimizer=optimizer, layer = 2, loss_fn=loss_fn, epochs = 30)\n",
    "\n",
    "    newXTrain = encoder_model.encode(dataset_train, 0) #Pass xTest through the encoder\n",
    "    newXTest = encoder_model.encode(dataset_test, 0) #Pass xTest through the encoder\n",
    "    \n",
    "    # Test and get the resulting predicted y values\n",
    "    if multi:\n",
    "        aeLogRegModel = OneVsRestClassifier(LogisticRegression()).fit(newXTrain, ytrain)\n",
    "    else:\n",
    "        aeLogRegModel = LogisticRegression().fit(newXTrain, ytrain)\n",
    "    \n",
    "    y_predict_ae = aeLogRegModel.predict(newXTest)\n",
    "    return y_predict_ae\n",
    "\n",
    "\n",
    "aeResults = []\n",
    "yActual = np.array([[y_names[np.argmax(vector[0:-3])], 1 if vector[-3]==1 else 0, 1 if vector[-2]==1 else 0, 1 if vector[-1]==1 else 0] for vector in yTest])\n",
    "sizes = [\n",
    "         (int(len(xTest[0])/5), int(len(xTest[0])/10))]\n",
    "'''\n",
    "print(\"Results logreg without autoencoder:\")\n",
    "startTime = time.time()\n",
    "aeLogRegModel = OneVsRestClassifier(LogisticRegression()).fit(xTrain, np.array(yTrain))\n",
    "yPred = aeLogRegModel.predict(xTest)\n",
    "elapsedTime = time.time() - startTime\n",
    "yPred = np.array([[y_names[np.argmax(vector[0:-3])], 1 if vector[-3]>=0.5 else 0, 1 if vector[-2]>=0.5 else 0, 1 if vector[-1]>=0.5 else 0] for vector in yPred])\n",
    "\n",
    "isCorrect = [[yPred[i][j] == yActual[i][j] for j in range(len(yPred[0]))] for i in range(len(yPred))]\n",
    "\n",
    "\n",
    "numCorrect = np.sum(isCorrect, axis=0)\n",
    "\n",
    "pred0, act0 = [v[0] for v in yPred], [v[0] for v in yActual]\n",
    "pred1, act1 =  [int(v[1]) for v in yPred], [v[1] for v in yActual]\n",
    "pred2, act2 =  [int(v[2]) for v in yPred], [v[2] for v in yActual]\n",
    "pred3, act3 =  [int(v[3]) for v in yPred], [v[3] for v in yActual]\n",
    "\n",
    "##### Write output file and get f1 score\n",
    "pd.DataFrame({\n",
    "    \"TumorTypePrediction\": pred0,\n",
    "    \"TP53MutationPrediction\": pred1,\n",
    "    \"KRASMutationPrediction\": pred2,\n",
    "    \"BRAFMutationPrediction\": pred3,\n",
    "}).to_csv(\"test_predictions.tsv\", sep=\"\\t\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"primary.disease.or.tissue\": act0,\n",
    "    \"TP53_mutant\": act1,\n",
    "    \"KRAS_mutant\": act2,\n",
    "    \"BRAF_mutant\": act3,\n",
    "}).to_csv(\"test_actuals.tsv\", sep=\"\\t\")\n",
    "\n",
    "print(\"% Correct with logreg : \", (numCorrect/len(isCorrect)))\n",
    "print(\"Elapsed time: %f s\" % elapsedTime)\n",
    "!Rscript class/BME230_F1score_V2.R test_predictions.tsv test_actuals.tsv\n",
    "'''\n",
    "print()\n",
    "print(\"results logreg with autoencoder\")\n",
    "for size in sizes:\n",
    "    size1, size2 = size\n",
    "    \n",
    "    startTime = time.time()\n",
    "    yPred = run_logreg(xTrain, np.array(yTrain), np.array(X_sub_holdout), np.array(X_sub_holdout), size1, size2, True)\n",
    "    elapsedTime = time.time() - startTime\n",
    "    \n",
    "    yPred = np.array([[y_names[np.argmax(vector[0:-3])], 1 if vector[-3]>=0.5 else 0, 1 if vector[-2]>=0.5 else 0, 1 if vector[-1]>=0.5 else 0] for vector in yPred])\n",
    "    \n",
    "    isCorrect = [[yPred[i][j] == yActual[i][j] for j in range(len(yPred[0]))] for i in range(len(yPred))]\n",
    "\n",
    "\n",
    "    numCorrect = np.sum(isCorrect, axis=0)\n",
    "    \n",
    "    pred0, act0 = [v[0] for v in yPred], [v[0] for v in yActual]\n",
    "    pred1, act1 =  [int(v[1]) for v in yPred], [v[1] for v in yActual]\n",
    "    pred2, act2 =  [int(v[2]) for v in yPred], [v[2] for v in yActual]\n",
    "    pred3, act3 =  [int(v[3]) for v in yPred], [v[3] for v in yActual]\n",
    "\n",
    "    ##### Write output file and get f1 score\n",
    "    pd.DataFrame({\n",
    "        \"TumorTypePrediction\": pred0,\n",
    "        \"TP53MutationPrediction\": pred1,\n",
    "        \"KRASMutationPrediction\": pred2,\n",
    "        \"BRAFMutationPrediction\": pred3,\n",
    "    }).to_csv(\"predict.tsv\", sep=\"\\t\")\n",
    "    \n",
    "    pd.DataFrame({\n",
    "        \"primary.disease.or.tissue\": act0,\n",
    "        \"TP53_mutant\": act1,\n",
    "        \"KRAS_mutant\": act2,\n",
    "        \"BRAF_mutant\": act3,\n",
    "    }).to_csv(\"test_actuals.tsv\", sep=\"\\t\")\n",
    "\n",
    "    print(\"% Correct with logreg and autoencoder: \", size1, size2, (numCorrect/len(isCorrect)))\n",
    "    print(\"Elapsed time: %fs\" % elapsedTime)\n",
    "    !Rscript class/BME230_F1score_V2.R test_predictions.tsv test_actuals.tsv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Logreg:\n",
    "Without ae: \n",
    "[0.93544601 0.79049296 0.95011737 0.95892019]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% Correct with logreg and autoencoder:  1135 567 [0.89553991 0.77053991 0.94776995 0.9536385 ]\n",
    "Elapsed time: 3165.212040s\n",
    "[1] \"Glioblastoma Multiforme_F1_score: 0.868421052631579\"\n",
    "[1] \"Skin Cutaneous Melanoma_F1_score: 0.973913043478261\"\n",
    "[1] \"Head & Neck Squamous Cell Carcinoma_F1_score: 0.945454545454545\"\n",
    "[1] \"Breast Invasive Carcinoma_F1_score: 0.995049504950495\"\n",
    "[1] \"Lung Adenocarcinoma_F1_score: 0.87719298245614\"\n",
    "[1] \"Brain Lower Grade Glioma_F1_score: 0.938967136150235\"\n",
    "[1] \"Stomach Adenocarcinoma_F1_score: 0.775\"\n",
    "[1] \"Kidney Clear Cell Carcinoma_F1_score: 0.957746478873239\"\n",
    "[1] \"Liver Hepatocellular Carcinoma_F1_score: 0.967741935483871\"\n",
    "[1] \"Bladder Urothelial Carcinoma_F1_score: 0.857142857142857\"\n",
    "[1] \"Ovarian Serous Cystadenocarcinoma_F1_score: 0.971428571428571\"\n",
    "[1] \"Esophageal Carcinoma_F1_score: 0.771428571428571\"\n",
    "[1] \"Thyroid Carcinoma_F1_score: 1\"\n",
    "[1] \"Lung Squamous Cell Carcinoma_F1_score: 0.811188811188811\"\n",
    "[1] \"Uveal Melanoma_F1_score: 1\"\n",
    "[1] \"Kidney Papillary Cell Carcinoma_F1_score: 0.902654867256637\"\n",
    "[1] \"Sarcoma_F1_score: 0.847058823529412\"\n",
    "[1] \"Cervical & Endocervical Cancer_F1_score: 0.87719298245614\"\n",
    "[1] \"Mesothelioma_F1_score: 0.96969696969697\"\n",
    "[1] \"Acute Myeloid Leukemia_F1_score: 0.315789473684211\"\n",
    "[1] \"Rectum Adenocarcinoma_F1_score: 0.357142857142857\"\n",
    "[1] \"Prostate Adenocarcinoma_F1_score: 1\"\n",
    "[1] \"Thymoma_F1_score: 0.974358974358974\"\n",
    "[1] \"Colon Adenocarcinoma_F1_score: 0.795918367346939\"\n",
    "[1] \"Pheochromocytoma & Paraganglioma_F1_score: 1\"\n",
    "[1] \"Kidney Chromophobe_F1_score: 0.782608695652174\"\n",
    "[1] \"Adrenocortical Cancer_F1_score: 0.962962962962963\"\n",
    "[1] \"Uterine Corpus Endometrioid Carcinoma_F1_score: 0.833333333333333\"\n",
    "[1] \"Uterine Carcinosarcoma_F1_score: 0.533333333333333\"\n",
    "[1] \"Pancreatic Adenocarcinoma_F1_score: 0.947368421052632\"\n",
    "[1] \"Testicular Germ Cell Tumor_F1_score: 0.964285714285714\"\n",
    "[1] \"Cholangiocarcinoma_F1_score: 0.545454545454545\"\n",
    "[1] \"Diffuse Large B-Cell Lymphoma_F1_score: 1\"\n",
    "[1] \"Overall Tissue F1 score: 0.872593245807006\"\n",
    "[1] \"TP53_F1_score: 0.621490803484995\"\n",
    "[1] \"KRAS_F1_score: 0.485549132947977\"\n",
    "[1] \"BRAF_F1_score: 0.635944700460829\"\n",
    "[1] \"Mutation_F1_score: 0.582914113460163\"\n",
    "[1] \"Final_F1_score: 0.728314866158979\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
