{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BME-230B: Homework 1\n",
    "\n",
    "Your assignment for homework 1 is to redo the linear regression analysis, but using a classification method from SKLearn.\n",
    "\n",
    "Goals and Requirements:\n",
    "1. Select a classification method from [SKLearn](http://scikit-learn.org/)\n",
    "    1. I'd recommend logistics regression or any forest method as they are more intuitive. SVM would be a much more difficult method to understand.\n",
    "2. Write a short explanation of the method and how it works (look for explanations in documention, youtube, or online).\n",
    "3. Try to achieve the highest accuracy / estimator quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "Method Selected: Random Forest Regressor\n",
    "\n",
    "#### Short Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Create training/test splits and train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and Split input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint as spRand\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Parse input\n",
    "df = pd.read_csv(\"data/breast-cancer-wisconsin.data.csv\")\n",
    "\n",
    "# Re-encode values\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "for col in df.columns:\n",
    "    df[col] = encoder.fit_transform(df[col])\n",
    "    \n",
    "#Split data set into train/test\n",
    "(train, test) = train_test_split(df, test_size=0.2)    \n",
    "features = list(df.keys())\n",
    "features.remove('id')\n",
    "features.remove('class')\n",
    "features.remove('mitoses')\n",
    "features.remove('clump-thickness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uniformity-of-cell-size', 'uniformity-of-cell-shape', 'marginal-adhesion', 'single-epithelial-cell-size', 'bare-nuclei', 'bland-chromatin', 'normal-nucleoli']\n"
     ]
    }
   ],
   "source": [
    "x = train[features]\n",
    "y = train['class']\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Hyperparameters, Train Best Estimator, Test training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulk Feature Score 0.9660107334525939\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# use RandomizedSearchCV to randomly probe hyperparameters, output is highest scoring param_dist\n",
    "def randomCV(features):\n",
    "    clf = RandomForestClassifier()\n",
    "    param_dist = {'bootstrap' : [False, True], \n",
    "                      'criterion' : ['gini','entropy'],\n",
    "                      'max_depth' : [3 , None], \n",
    "                      'max_features' : spRand(1,len(features) + 1),\n",
    "                      'min_samples_leaf' : spRand(1,11), \n",
    "                      'min_samples_split' : spRand(2,11), \n",
    "                      'n_estimators' : spRand(6, 128),\n",
    "                      'warm_start' : [False, True]}\n",
    "\n",
    "    n_iter_search = 16\n",
    "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search)\n",
    "    return(random_search)\n",
    "# Use every feature\n",
    "\n",
    "random_search = randomCV(features)\n",
    "random_search.fit(x, y)\n",
    "fitScore = random_search.score(x,y)\n",
    "print('Bulk Feature Score', fitScore)\n",
    "\n",
    "y_pred = random_search.best_estimator_.predict(test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predOutPut(features, y_pred):   \n",
    "    test_out = test['class']\n",
    "    featOut = []\n",
    "    print('Prediction with test data set including:', features)\n",
    "    print('Accuracy', accuracy_score(test_out, y_pred))\n",
    "    print(confusion_matrix(test_out, y_pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(test_out, y_pred).ravel()\n",
    "    fpr = float(fp) / (fp+tn)\n",
    "    fnr = float(fn) / (tp+fn)\n",
    "    print(\"False positive rate: (predicting malignant while benign)\", fpr)\n",
    "    print(\"False negative rate: (predicting benign while malignant)\", fnr)\n",
    "    print('Feature contribution as normalized to 1')\n",
    "    featRank = list(zip(train[features], random_search.best_estimator_.feature_importances_))\n",
    "    featRank.sort(key=lambda x: x[1], reverse=True)\n",
    "    for feat in featRank:\n",
    "        print(feat[0], feat[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with test data set including: ['uniformity-of-cell-size', 'uniformity-of-cell-shape', 'marginal-adhesion', 'single-epithelial-cell-size', 'bare-nuclei', 'bland-chromatin', 'normal-nucleoli']\n",
      "Accuracy 0.9285714285714286\n",
      "[[79  7]\n",
      " [ 3 51]]\n",
      "False positive rate: (predicting malignant while benign) 0.08139534883720931\n",
      "False negative rate: (predicting benign while malignant) 0.05555555555555555\n",
      "Feature contribution as normalized to 1\n",
      "uniformity-of-cell-size 0.2348686816112566\n",
      "single-epithelial-cell-size 0.18267169456425394\n",
      "uniformity-of-cell-shape 0.17033922829548756\n",
      "normal-nucleoli 0.12364416907209613\n",
      "marginal-adhesion 0.12294417416392453\n",
      "bare-nuclei 0.08325964310354993\n",
      "bland-chromatin 0.08227240918943142\n"
     ]
    }
   ],
   "source": [
    "predOutPut(features, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "What feature contributes most to the prediction? How can we tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniformity measurements (cell size, cell shape) consistently provide the highest contribution to this predictive model when all feature are used. When low performing features are removed, their positions are less certain, but still often quite high. This is provided as an attribute of the classifier object by sci-kit learn, and is determined by the mean decrease impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain in your own words the difference between regression and classification methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification methods seem to produce discrete results that are direct predictions of class label. Regression, on the other hand, produces continuous values that must be sorted into classes thereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it best to use all the features or exclude some? Why do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual exclusion of low performing features seemed to improve accuracy and confusion matrix results. If the features do not actually possess predictive value, including them in the classification provides the chance for erroneous classification. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
